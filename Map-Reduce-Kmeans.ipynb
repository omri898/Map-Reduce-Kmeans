{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbfb2b7-d45b-403c-9a3a-4a25854e913d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Map-Reudce Kmeans\n",
    "\n",
    "### Using Spark RDD and parallel work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0fadb37-488e-4425-aa6e-e564049ee874",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35942dc5-d35b-4628-bf84-68b19ed9dd75",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* The map function takes an RDD of all the points and a set of k centroids.\n",
    "* Calculates the closest centroid for each point using euclidean distance.\n",
    "* returns < c , p > pairs where c is the closest centroid to point p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0ab584-4b99-4e9e-8c27-8a7e18beec31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def euc_dist(p1,p2):\n",
    "    # Calculates the euclidean distance between 2 points\n",
    "    return sum((value1 - value2) ** 2 for value1, value2 in zip(p1, p2)) ** 0.5\n",
    "\n",
    "def assign_point_to_centroid(point, centroids):\n",
    "# Function that assigns the closes centroid to a point\n",
    "# The function to map by\n",
    "    closest_centroid = None\n",
    "    closest_distance = float('inf')\n",
    "    for centroid in centroids:\n",
    "        distance = euc_dist(point, centroid)\n",
    "        if distance < closest_distance:\n",
    "            closest_distance = distance\n",
    "            closest_centroid = centroid\n",
    "    return (closest_centroid, point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921b5fa1-7b5c-4e10-80ec-4a5445cfaa55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def map_function(rdd, centroids):\n",
    "    # Assign each point to the closest centroid\n",
    "    assigned_points = rdd.map(lambda point: assign_point_to_centroid(point,centroids))\n",
    "\n",
    "    # Collect the assigned points as a list of tuples\n",
    "    # assigned_points_list = assigned_points.collect()\n",
    "\n",
    "    return assigned_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "883aec4f-1951-4618-82eb-36d8d5009ea8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae823f3-085b-4836-8d5c-60b99ac315da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* The reduce funtion takes the output from the map function.\n",
    "* Calculates the new centroid by averaging all the points in each cluster.\n",
    "* Returns < old_centroid , new_centroid > pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1e2aa3-e06d-4a14-afdd-8b705a8febe3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fucntion that calculates new centroids based on points assigned to it\n",
    "def calculate_new_centroids(points):\n",
    "    points_list = list(points)\n",
    "    num_points = len(points_list)\n",
    "    dimensions = len(points_list[0]) # 0 is arbitrary\n",
    "    centroid_sum = [0.0] * dimensions\n",
    "\n",
    "    for point in points_list:\n",
    "        centroid_sum = [centroid_sum[i] + point[i] for i in range(dimensions)]\n",
    "\n",
    "    new_centroid = tuple([centroid_sum[i] / num_points for i in range(dimensions)])\n",
    "    return new_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb73e12c-9f6c-4180-8a52-ba30d48b6456",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reduce_function(assigned_points):\n",
    "\n",
    "    # Turn points to tuples\n",
    "    assigned_points = [(tuple(t[0]),tuple(t[1])) for t in assigned_points.collect()]\n",
    "\n",
    "    # Create an RDD from assigned_points\n",
    "    assigned_points_rdd = sc.parallelize(assigned_points)\n",
    "\n",
    "    # Group assigned points by centroid (shuffle and sort)\n",
    "    grouped_points = assigned_points_rdd.groupByKey()\n",
    "\n",
    "    # Calculate new centroids - This is a list of len k of (old_centroid,new_centroid)\n",
    "    old_new_centroids = grouped_points.mapValues(calculate_new_centroids)\n",
    "    \n",
    "    return old_new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdf68635-ce88-4205-8d9f-1ae9efd07227",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## K-Means Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3809682d-e93b-43dc-a119-ddb95a147f24",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "* In the Kmeans algorithm, map and reduce functions are repeated until convergence is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1076035e-07e3-49f3-94fc-0514553086dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Choosing K random points for the start of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77d969e0-cf93-42fe-bd91-debe266485b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def choose_centroids(rdd,K):\n",
    "    # Randomly select k points from the RDD\n",
    "    centroids = rdd.takeSample(False, K)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f31615e-9f8a-4b35-a9d3-5dc7ba6a8a42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Kmeans(rdd, k, CT=0.0001, I=30):\n",
    "    '''\n",
    "    rdd - The data points\n",
    "    k - The number of clusters\n",
    "    CT - Convergence threshold (parameter - default is set to 0.0001)\n",
    "    I - Number of iterations for K-Means (parameter - default is set to 30)\n",
    "    '''\n",
    "    # Choose random points for the start of the algorithm\n",
    "    centroids = choose_centroids(rdd, k)\n",
    "    for i in range(I):\n",
    "        # Map points to centroids\n",
    "        assigned_points = map_function(rdd,centroids)\n",
    "\n",
    "        # Calculate new centroids\n",
    "        old_new_centroids = reduce_function(assigned_points)\n",
    "\n",
    "        # Check for convergence\n",
    "        old_new_centroids_list = old_new_centroids.collect()\n",
    "        count = 0\n",
    "        for (old_centroid, new_centroid) in old_new_centroids_list:\n",
    "            if euc_dist(old_centroid,new_centroid) <= CT:\n",
    "                count += 1\n",
    "        if count == k:\n",
    "            break\n",
    "        centroids = [np.array(c[1]) for c in old_new_centroids_list]\n",
    "        \n",
    "    return centroids\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7d4ff4-4ba0-4b30-a1e6-cf04411e7193",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Testing the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82a0bc51-8fbd-4620-b2e8-eb7dc1975fab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I will be using the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6444e171-ffa5-45e8-9fa3-6c135cf504c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset_name = \"iris\"\n",
    "iris_path = f'/FileStore/tables/iris.csv'\n",
    "iris_df = spark.read.csv(iris_path, header= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a733ebd-3169-4b3b-9e52-6cac42415894",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+-----+\n| f1| f2| f3| f4|class|\n+---+---+---+---+-----+\n|5.1|3.5|1.4|0.2|    0|\n|4.9|  3|1.4|0.2|    0|\n|4.7|3.2|1.3|0.2|    0|\n|4.6|3.1|1.5|0.2|    0|\n|  5|3.6|1.4|0.2|    0|\n+---+---+---+---+-----+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "iris_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3ff071c-7bd6-440c-a0f2-c6caff676d22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Preproccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaf52057-90c7-47a8-b3f3-e40f784a4b26",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Changing data types and normalizing attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d392360-a4ba-482b-861a-aeb4c14f1a2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def preprocess(df):\n",
    "    # Preprocesses the data. Returns an RDD.\n",
    "\n",
    "    # Select columns to use for Kmeans algorithm\n",
    "    selected_columns = df.columns[:-1]\n",
    "    df = df.select(selected_columns)\n",
    "\n",
    "    # Transform values to float type\n",
    "    for c in selected_columns:\n",
    "        df = df.withColumn(c, functions.col(c).cast('float'))\n",
    "    \n",
    "    # Normalize each column\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df.collect())\n",
    "    df = scaler.transform(df.collect())\n",
    "    rdd = sc.parallelize(df)\n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1499d0-0d8c-4769-b506-e1bfd455ce8e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Running The Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49a8742d-b69b-4676-8d9e-81a6a7fd3e36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [array([0.2655555 , 0.70999997, 0.08      , 0.07333334]),\n array([0.15624996, 0.40234374, 0.15095339, 0.12109375]),\n array([0.5663082 , 0.37903224, 0.68069982, 0.67697132])]"
     ]
    }
   ],
   "source": [
    "rdd = preprocess(iris_df)\n",
    "Kmeans(rdd,k=3,I=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e794c9a-3390-4c66-abb1-89eacd40585e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17ad1a47-5ee8-43e2-b712-46c5d7664dd4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Evaluating the model with:\n",
    "* Calinski Harabasz Score (CH)\n",
    "* Adjusted Rand Index (ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a6447c-95d8-4a6d-b7b8-adb334e58b7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "def eval_CH(points,centroids):\n",
    "    # Calculate the Calinski-Harabasz score\n",
    "    assignments = []\n",
    "    d = lambda p1,p2: sum((a - b) ** 2 for a, b in zip(p1, p2)) ** 0.5\n",
    "    for point in points:\n",
    "        distances = [d(point,centroid) for centroid in centroids]\n",
    "        smallest_value_index = distances.index(min(distances))\n",
    "        assignments.append(smallest_value_index)\n",
    "    score = calinski_harabasz_score(points, assignments)\n",
    "    return score\n",
    "\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "def eval_ARI(points, truth_classes, centroids):\n",
    "    # Calculate the Adjusted Rand Index (ARI) score\n",
    "    assignments = []\n",
    "    d = lambda p1,p2: sum((a - b) ** 2 for a, b in zip(p1, p2)) ** 0.5\n",
    "    for point in points:\n",
    "        distances = [d(point,centroid) for centroid in centroids]\n",
    "        smallest_value_index = distances.index(min(distances))\n",
    "        assignments.append(smallest_value_index)\n",
    "    ari_score = adjusted_rand_score(truth_classes, assignments)\n",
    "    return ari_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd4edf7-e00c-4ca1-98e4-4a727974e5b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def true_classes(df):\n",
    "    return np.array(df.select(df.columns[-1]).rdd.flatMap(lambda x: x).collect())\n",
    "\n",
    "def calculate_evaluation_statistics(ch_values, ari_values):\n",
    "    # Calculate average and standard deviation for CH values\n",
    "    ch_average = np.average(ch_values)\n",
    "    ch_std_dev = np.std(ch_values)\n",
    "\n",
    "    # Calculate average and standard deviation for ARI values\n",
    "    ari_average = np.average(ari_values)\n",
    "    ari_std_dev = np.std(ari_values)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"CH Values:\")\n",
    "    print(\"  Average:\", ch_average)\n",
    "    print(\"  Standard Deviation:\", ch_std_dev)\n",
    "\n",
    "    print(\"ARI Values:\")\n",
    "    print(\"  Average:\", ari_average)\n",
    "    print(\"  Standard Deviation:\", ari_std_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2deab59d-feb7-4abd-9e90-df67befa74fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Running multiple experiments of Kmeans and testing different k values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573332a2-5866-452f-964f-8acec75cd0aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def evaluateKmeans(df, k, CT=0.0001, I=30, exp=10):\n",
    "    '''\n",
    "    df - The data including the class column\n",
    "    k - The number of clusters for each expreiment\n",
    "    CT - Convergence threshold (parameter - default is set to 0.0001)\n",
    "    I - Number of iterations for K-Means (parameter - default is set to 30)\n",
    "    Exp - Number of Experiments (parameter - default is set to 10)\n",
    "    '''\n",
    "    \n",
    "    ch_values = []\n",
    "    ari_values = []\n",
    "    rdd = preprocess(df)\n",
    "    for i in range(exp):\n",
    "        centroids = Kmeans(rdd,k,CT,I)\n",
    "        points = np.array(rdd.collect())\n",
    "        ch_values.append(eval_CH(points,centroids))\n",
    "        true_classifictaions = true_classes(df)\n",
    "        ari_values.append(eval_ARI(points, true_classifictaions, centroids))\n",
    "    calculate_evaluation_statistics(ch_values, ari_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1350a6d3-5c12-4b0b-8da8-3c847c482d6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 2\nCH Values:\n  Average: 353.3674059572578\n  Standard Deviation: 5.684341886080802e-14\nARI Values:\n  Average: 0.5681159420289854\n  Standard Deviation: 1.1102230246251565e-16\nk = 3\nCH Values:\n  Average: 308.7014828336743\n  Standard Deviation: 68.83684373816534\nARI Values:\n  Average: 0.6239346481112749\n  Standard Deviation: 0.1278073990035785\nk = 4\nCH Values:\n  Average: 289.97391483091803\n  Standard Deviation: 19.81000400718611\nARI Values:\n  Average: 0.5888747472530399\n  Standard Deviation: 0.028079484887737507\nk = 5\nCH Values:\n  Average: 261.29696195714763\n  Standard Deviation: 21.38396727253739\nARI Values:\n  Average: 0.5845928449391831\n  Standard Deviation: 0.07575087350687562\nk = 6\nCH Values:\n  Average: 236.25615362598145\n  Standard Deviation: 23.389486253440136\nARI Values:\n  Average: 0.5207243692274759\n  Standard Deviation: 0.06788437146538576\n"
     ]
    }
   ],
   "source": [
    "for k in range (2,7):\n",
    "    print(f\"k = {k}\")\n",
    "    evaluateKmeans(iris_df,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85c02f18-542e-4352-88af-ce085ea9da5a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see the model performs best on k=2 and k=3.\n",
    "\n",
    "* The highest average CH score is at 2 clusters which indicates better-defined clusters with good separation and compactness.\n",
    "* The highest average ARI is at 3 clusters which indicates better agreement between the predicted labels and the true labels of the data."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 660097145653111,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BigData2 - Kmeans",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
